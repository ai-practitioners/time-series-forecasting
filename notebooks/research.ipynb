{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales - Time Series Forecasting\n",
    "\n",
    "Use machine learning to predict grocery sales. [source](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/description)\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this Kaggle competition, the goal is to \n",
    "\n",
    "> build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.\n",
    "\n",
    "The evaluation metric for this competition is ***Root Mean Squared Logarithmic Error***.\n",
    "\n",
    "The `RMSLE` is calculated as:\n",
    "\n",
    "$$\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ n $ is the total number of instances,\n",
    "     \n",
    "- $\\hat{y}$ is the predicted value of the target for instance (i),\n",
    "   \n",
    "- $y_i$ is the actual value of the target for instance (i), and,\n",
    " \n",
    "- $log$ is the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each id in the test set, you must predict a value for the sales variable. The file should contain a header and have the following format:\n",
    "\n",
    "    ```\n",
    "    id,sales\n",
    "    3000888,0.0\n",
    "    3000889,0.0\n",
    "    3000890,0.0\n",
    "    3000891,0.0\n",
    "    3000892,0.0\n",
    "    etc.\n",
    "    ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries for this research notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# to overcome path issue for src\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# set the path to the current file\n",
    "current_file_path = Path().resolve()\n",
    "print(f\"current_file_path is {current_file_path}\")\n",
    "\n",
    "# set the path to the src folder\n",
    "src_folder_path = current_file_path.parent / 'src'\n",
    "print(f\"src_folder_path is {src_folder_path}\")\n",
    "\n",
    "# add the src folder to the system path\n",
    "sys.path.append(str(src_folder_path))\n",
    "\n",
    "from data_loader import DBDataLoader\n",
    "from logger import logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Query data from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data set using .sql file\n",
    "query_file_path = '../src/scripts/train_store_hols.sql'\n",
    "\n",
    "db = DBDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(query_file_path, 'r') as query:\n",
    "    chunks = db.load(query=query.read())\n",
    "    # count += 1\n",
    "    print(f'chunks size: {sys.getsizeof(chunks)}')\n",
    "    logging.info(f\"chunks loaded {sys.getsizeof(chunks)}\")\n",
    "    df = pd.DataFrame()\n",
    "    for i in tqdm(range(sys.getsizeof(chunks)), desc='Reading from DB'):\n",
    "        for chunk in chunks:\n",
    "            df = pd.concat([df, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF loaded confirm: 1972674 rows × 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt='select * from VwDump1'\n",
    "\n",
    "chunks = db.load(query=stmt)\n",
    "\n",
    "print(f'VwDump1 chunks size: {sys.getsizeof(chunks)}')\n",
    "logging.info(f\"VwDump1 chunks loaded {sys.getsizeof(chunks)}\")\n",
    "\n",
    "view_df = pd.DataFrame()\n",
    "for i in tqdm(range(sys.getsizeof(chunks)), desc='Reading from View'):\n",
    "    for chunk in chunks:\n",
    "        view_df = pd.concat([view_df, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF loaded confirm: 3000888 rows × 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "view_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out data ingestion with connectorx library\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create engine to talk to database\n",
    "engine = create_engine(\n",
    "    f'mysql+pymysql://'             # dialect + driver\n",
    "    f'{config.get(\"USERNAME\")}'     # username\n",
    "    f':{config.get(\"PASSWORD\")}'    # password\n",
    "    f'@{config.get(\"ENDPOINT\")}'    # host\n",
    "    f':{config.get(\"PORT\")}'        # port\n",
    "    f'/{config.get(\"DBNAME\")}'      # database\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish connection and make the query\n",
    "with engine.connect() as cnxn:\n",
    "    with open('../src/scripts/query_data.sql') as f:\n",
    "        query = text(f.read())\n",
    "        results = pd.read_sql(query, cnxn)\n",
    "\n",
    "# runtime 1 min 1.4 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = os.getenv(\"PS_HOST\")\n",
    "DB_USERNAME = os.getenv(\"PS_USERNAME\")\n",
    "DB_PASSWORD = os.getenv(\"PS_PASSWORD\")\n",
    "DB_DATABASE = os.getenv(\"PS_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    host=DB_HOST,\n",
    "    user=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    database=DB_DATABASE,\n",
    "    # ssl_verify_identity=True,\n",
    "    # ssl_ca=\"/etc/ssl/certs/ca-certificates.crt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "\n",
    "batch_size = 100000\n",
    "start_id = 0\n",
    "rows = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    cursor.execute(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM train\n",
    "        WHERE id >= {start_id}\n",
    "        ORDER BY id\n",
    "        LIMIT {batch_size}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    batch = cursor.fetchall()\n",
    "    if not batch:\n",
    "        break\n",
    "    \n",
    "    rows.extend(batch)\n",
    "    start_id = batch[-1][0] + 1\n",
    "    \n",
    "    if len(rows) >= 3000000:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    host=DB_HOST,\n",
    "    user=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    database=DB_DATABASE,\n",
    "    # ssl_verify_identity=True,\n",
    "    # ssl_ca=\"/etc/ssl/certs/ca-certificates.crt\"\n",
    "    connect_timeout=1000\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SET GLOBAL connect_timeout=60;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SET WORKLOAD = 'olap';\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SELECT * FROM full_df\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(len(rows))\n",
    "\n",
    "df = pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_store = view_df.groupby(by=['store_nbr', 'family'], group_keys=True).agg('sum', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_store.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(view_df, title=\"ProfileReport view_df\")\n",
    "# profile.to_notebook_iframe()\n",
    "profile.to_file(\"../artifacts/reports/view_df_ProfileReport.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"ProfileReport defaults\")\n",
    "# profile.to_notebook_iframe()\n",
    "profile.to_file(\"../artifacts/reports/df_ProfileReport.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
