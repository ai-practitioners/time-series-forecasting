{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales - Time Series Forecasting\n",
    "\n",
    "Use machine learning to predict grocery sales. [source](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/description)\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this Kaggle competition, the goal is to \n",
    "\n",
    "> build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.\n",
    "\n",
    "The evaluation metric for this competition is ***Root Mean Squared Logarithmic Error***.\n",
    "\n",
    "The `RMSLE` is calculated as:\n",
    "\n",
    "$$\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ n $ is the total number of instances,\n",
    "     \n",
    "- $\\hat{y}$ is the predicted value of the target for instance (i),\n",
    "   \n",
    "- $y_i$ is the actual value of the target for instance (i), and,\n",
    " \n",
    "- $log$ is the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each id in the test set, you must predict a value for the sales variable. The file should contain a header and have the following format:\n",
    "\n",
    "    ```\n",
    "    id,sales\n",
    "    3000888,0.0\n",
    "    3000889,0.0\n",
    "    3000890,0.0\n",
    "    3000891,0.0\n",
    "    3000892,0.0\n",
    "    etc.\n",
    "    ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries for this research notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# to overcome path issue for src\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# set the path to the current file\n",
    "current_file_path = Path().resolve()\n",
    "print(f\"current_file_path is {current_file_path}\")\n",
    "\n",
    "# set the path to the src folder\n",
    "src_folder_path = current_file_path.parent / 'src'\n",
    "print(f\"src_folder_path is {src_folder_path}\")\n",
    "\n",
    "# add the src folder to the system path\n",
    "sys.path.append(str(src_folder_path))\n",
    "\n",
    "from data_loader import DBDataLoader\n",
    "import data_cleaner as dc\n",
    "import data_exploration as eda\n",
    "import helper as helper\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Query data from local MySQL using SQLAlchemy and Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define connection string for local database\n",
    "conn = DBDataLoader().get_connection_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query all records from full_df table\n",
    "query = 'SELECT * FROM full_df'\n",
    "\n",
    "# load in table using sqlalchemy and polars\n",
    "results = pl.read_database_uri(query=query, uri=conn, engine=\"connectorx\")\n",
    "\n",
    "print(f'Size of dataframe ingested: {round(results.estimated_size(\"mb\"),2)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of queried results - reduce the need to query and wait.\n",
    "df = results.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (Polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 verbs get most job done\n",
    "# select/slice columns -> select\n",
    "# create/transform/assign columns -> with_columns\n",
    "# filter/slice/query rows -> filter\n",
    "# join/merge another dataframe -> join\n",
    "# group dataframe rows -> groupby\n",
    "# aggregate groups -> agg\n",
    "# sort dataframe -> sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor clean up - clean up sql query output as a result from sql join statements\n",
    "df = dc.query_clean_up(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downcast columns datatypes to reduce dataframe memory\n",
    "df = dc.shrink_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot temporal average of sales\n",
    "eda.plot_sales_averages(dataframe=df, select_hierarchy='city', name='Babahoyo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot an inventory heat map for all stores\n",
    "eda.plot_stores_inventory(dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the count of products sold for all stores\n",
    "eda.plot_prod_count_per_store(dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories each family into groups\n",
    "departments = ['grocery', 'household', 'apparel', 'alcohol', 'entertainment', 'kids']\n",
    "\n",
    "grocery = [\n",
    "    'GROCERY I', 'GROCERY II', 'BEVERAGES', 'BREAD/BAKERY', 'CANNED GOODS',\n",
    "    'CEREAL', 'DELI', 'EGGS', 'FROZEN FOODS', 'MEATS',\n",
    "    'POULTRY', 'PREPARED FOODS', 'DAIRY', 'SEAFOOD'\n",
    "]\n",
    "\n",
    "household = [\n",
    "    'CLEANING', 'HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES', 'LAWN AND GARDEN',\n",
    "    'PET SUPPLIES', 'HARDWARE'\n",
    "]\n",
    "\n",
    "apparel = ['LADIESWEAR', 'LINGERIE', 'BEAUTY', 'PERSONAL CARE']\n",
    "\n",
    "alcohol = ['LIQUOR,WINE,BEER']\n",
    "\n",
    "entertainment = ['BOOKS', 'MAGAZINES', 'PLAYERS AND ELECTRONICS']\n",
    "\n",
    "kids = ['BABY CARE', 'TOYS', 'SCHOOL AND OFFICE SUPPLIES', 'CELEBRATION']\n",
    "\n",
    "# store the groupings in a dictionary\n",
    "inventory = {\n",
    "    'grocery': grocery,\n",
    "    'household': household,\n",
    "    'apparel': apparel,\n",
    "    'alcohol': alcohol,\n",
    "    'entertainment': entertainment,\n",
    "    'kids': kids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df\n",
    "store = 1\n",
    "\n",
    "# filter dataframe by store\n",
    "store_df = dataframe.filter(pl.col('store_nbr') == store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# create a figure with 6 subplots\n",
    "departments_fig = make_subplots(\n",
    "    rows=6,\n",
    "    cols=2,\n",
    "    specs=[\n",
    "        # make subplot (1, 1) take 2 columns of space\n",
    "        [{'colspan': 2, 'rowspan': 2}, None],\n",
    "        [None, None],\n",
    "        [{'colspan': 2, 'rowspan': 2}, None],\n",
    "        [{}, {}],\n",
    "        [{}, {}],\n",
    "        [{}, {}] \n",
    "    ],\n",
    "    subplot_titles=[subplot_title.upper() for subplot_title in inventory.keys()]\n",
    ")\n",
    "\n",
    "# loop through each product in department_df\n",
    "for product in inventory['grocery']:\n",
    "    # filter store_df by elements of values in inventory dictionary\n",
    "    product_df = (\n",
    "        store_df\n",
    "        .filter(pl.col('family').is_in(inventory['grocery']))\n",
    "        .filter(pl.col('family') == product)\n",
    "        .sort(['family', 'date'])\n",
    "    )\n",
    "    \n",
    "    # build a line trace for each product\n",
    "    trace = go.Scatter(\n",
    "        x=product_df['date'],\n",
    "        y=product_df['sales'],\n",
    "        name=f'{product.upper()}',\n",
    "        legendgroup='grocery',\n",
    "        hovertemplate=f'{product.upper()}: %{{y:.2f}}'\n",
    "    )\n",
    "    \n",
    "    # add each product trace to the relevant subplot\n",
    "    departments_fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "# loop through each product in department_df\n",
    "for product in inventory['household']:\n",
    "    # filter store_df by elements of values in inventory dictionary\n",
    "    product_df = (\n",
    "        store_df\n",
    "        .filter(pl.col('family').is_in(inventory['household']))\n",
    "        .filter(pl.col('family') == product)\n",
    "        .sort(['family', 'date'])\n",
    "    )\n",
    "    \n",
    "    # build a line trace for each product\n",
    "    trace = go.Scatter(\n",
    "        x=product_df['date'],\n",
    "        y=product_df['sales'],\n",
    "        name=f'{product.upper()}',\n",
    "        legendgroup='household',\n",
    "        hovertemplate=f'{product.upper()}: %{{y}}'\n",
    "    )\n",
    "    \n",
    "    # add each product trace to the relevant subplot\n",
    "    departments_fig.add_trace(trace, row=3, col=1)\n",
    "    \n",
    "departments_fig.update_layout(\n",
    "    title=f\"Products' Sales for store {store}\",\n",
    "    template='plotly_dark',\n",
    "    height=1500,\n",
    ")\n",
    "\n",
    "departments_fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latitude and longitude values for each city\n",
    "eda.get_geo_coordinates('ggjx22', 'Quito', 'EC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (Time Series)\n",
    "\n",
    "Using a single family (product) of a store as practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = (\n",
    "    df\n",
    "    # filter out a sub set of dataset\n",
    "    .filter(pl.col('store_nbr') == 1)\n",
    "    .sort('family')\n",
    "    # further filter a single time series by a random family\n",
    "    .filter(pl.col('family') == \"AUTOMOTIVE\")\n",
    "    # convert to pandas to have a datetime index\n",
    "    .to_pandas()\n",
    "    # set column 'date' as index with a monthly freq\n",
    "    .set_index('date').asfreq('D')\n",
    "    # for simplicity of eda, consider target variable only\n",
    "    [['sales']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(data_frame=df_eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df.index.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_store = df.groupby(by=['store_nbr', 'family'], group_keys=True).agg('sum', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_store.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(df.to_pandas(), title=\"Time-Series EDA full df\")\n",
    "profile.to_notebook_iframe()\n",
    "profile.to_file(\"../artifacts/reports/full_df_ProfileReport.html\") # AttributeError: 'float' object has no attribute 'shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(11, 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sales_sum'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_plot = ['onpromotion_sum', 'transactions_sum', 'sales_sum']\n",
    "axes = df[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11, 9), subplots=True)\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Daily Totals (GWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013 = df.loc['2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_2013.loc['2013', 'sales_sum'].plot()\n",
    "ax.set_ylabel('Daily Sales for 2013');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='weekday', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='month', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='store_nbr', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.store_nbr.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- have outliers needing treatment; need to treat outliers first before we can analyse seasonality for `weekday` and `month`\n",
    "- selected Store 44 as it has most spread for `transaction_sum` meaning more activity; note assumption that 0 transaction means store is closed as there's no sale on that day\n",
    "- which tracks with 0 transactions occuring on days with holidays (not included to keep dataset small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str44 = df[(df.store_nbr == 44)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44 = df_str44.groupby(by=['date'], group_keys=True).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44.drop(columns=['id','store_nbr','year','month','day_of_month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44 = grp_df_str44.asfreq('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoML with pycaret\n",
    "\n",
    "EDA and ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check installed version\n",
    "import pycaret\n",
    "pycaret.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "\n",
    "# feature engineering on target variable (reduced regression models only)\n",
    "kwargs_target = {\n",
    "    'lag_feature':{\n",
    "        'lag':[1,2,3],\n",
    "        'mean':[[3]]\n",
    "    }\n",
    "}\n",
    "\n",
    "fe_target_rr = [WindowSummarizer(n_jobs=-1, truncate='bfill', **kwargs_target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.summarize import SlidingWindowSplitter\n",
    "import numpy as np \n",
    "\n",
    "# customized a sliding window cross validation object\n",
    "splitter = SlidingWindowSplitter(\n",
    "    fh=np.arange(1, 30+1),\n",
    "    window_length=365,\n",
    "    step_length=140\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "from pycaret.time_series import add_metric\n",
    "\n",
    "# create a custom function\n",
    "def rmsle(y_true, y_pred):\n",
    "    return mean_squared_log_error(y_true\n",
    "                        , y_pred\n",
    "                        , squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pycaret time series and init setup\n",
    "from pycaret.time_series import TSForecastingExperiment\n",
    "\n",
    "exp = TSForecastingExperiment()\n",
    "\n",
    "expt = exp.setup(\n",
    "  data=df_eda,\n",
    "  target='sales',\n",
    "  numeric_imputation_target='bfill',\n",
    "  scale_target='minmax',\n",
    "  fe_target_rr=fe_target_rr,\n",
    "  fold_strategy=splitter,\n",
    "  seasonal_period=[7,52,12,24,4,1],\n",
    "  sp_detection='auto',\n",
    "  num_sps_to_use=-1,\n",
    "  seasonality_type='auto',\n",
    "  enforce_exogenous=False,\n",
    "  n_jobs=-1,\n",
    "  session_id=42,\n",
    "  system_log=False,\n",
    "  log_experiment=False,\n",
    "  experiment_name='store_1_AUTOMOTIVE',\n",
    ")\n",
    "expt.add_metric('rmsle', 'RMSLE', rmsle, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.plot_model(plot='cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.check_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin autoML and return the top 3 models with lowest RMSLE\n",
    "best_models = expt.compare_models(\n",
    "    sort='RMSLE',\n",
    "    n_select=3,\n",
    "    turbo=False,\n",
    "    engine={'auto_arima':'statsforecast'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt.plot_model(estimator=best_models, plot='forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistical tests on original data\n",
    "check_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources for add_metric()\n",
    "\n",
    "- https://towardsdatascience.com/predict-customer-churn-the-right-way-using-pycaret-8ba6541608ac\n",
    "- https://github.com/pycaret/pycaret/issues/3491\n",
    "- https://github.com/pycaret/pycaret/issues/1063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# create a custom function\n",
    "def rmsle(y_true, y_pred):\n",
    "    return mean_squared_log_error(y_true\n",
    "                        , y_pred\n",
    "                        , squared=False)\n",
    "\n",
    "add_metric('msle', 'MSLE', mean_squared_log_error, greater_is_better=False) # default squared=True\n",
    "add_metric('rmsle', 'RMSLE', rmsle, greater_is_better=False) # for problem statement squared=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare baseline models\n",
    "best = compare_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
