{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales - Time Series Forecasting\n",
    "\n",
    "Use machine learning to predict grocery sales. [source](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/description)\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this Kaggle competition, the goal is to \n",
    "\n",
    "> build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.\n",
    "\n",
    "The evaluation metric for this competition is ***Root Mean Squared Logarithmic Error***.\n",
    "\n",
    "The `RMSLE` is calculated as:\n",
    "\n",
    "$$\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ n $ is the total number of instances,\n",
    "     \n",
    "- $\\hat{y}$ is the predicted value of the target for instance (i),\n",
    "   \n",
    "- $y_i$ is the actual value of the target for instance (i), and,\n",
    " \n",
    "- $log$ is the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each id in the test set, you must predict a value for the sales variable. The file should contain a header and have the following format:\n",
    "\n",
    "    ```\n",
    "    id,sales\n",
    "    3000888,0.0\n",
    "    3000889,0.0\n",
    "    3000890,0.0\n",
    "    3000891,0.0\n",
    "    3000892,0.0\n",
    "    etc.\n",
    "    ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries for this research notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra libraries for this version of notebook:\n",
    "- ydata-profiling\n",
    "- tqdm\n",
    "- connectorx\n",
    "- pycaret\n",
    "\n",
    "via below conda or pip install (or copy+paste into Terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install ydata-profiling tqdm connectorx pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ydata-profiling tqdm connectorx pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# to overcome path issue for src\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# set the path to the current file\n",
    "current_file_path = Path().resolve()\n",
    "print(f\"current_file_path is {current_file_path}\")\n",
    "\n",
    "# set the path to the src folder\n",
    "src_folder_path = current_file_path.parent / 'src'\n",
    "print(f\"src_folder_path is {src_folder_path}\")\n",
    "\n",
    "# add the src folder to the system path\n",
    "sys.path.append(str(src_folder_path))\n",
    "\n",
    "from data_loader import DBDataLoader\n",
    "from logger import logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Query data from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='SELECT * FROM quito_frm_view'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values()\n",
    "\n",
    "user = config.get('USERNAME')\n",
    "password = config.get('PASSWORD')\n",
    "host = config.get('ENDPOINT')\n",
    "db = config.get('DBNAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import connectorx as cx\n",
    "\n",
    "conn=f'mysql://{user}:{password}@{host}/{db}' \n",
    "df=cx.read_sql(conn, query) # , partition_on=\"id\", partition_num=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import MySQLdb\n",
    "\n",
    "engine = create_engine(f'mysql+mysqldb://{user}:{password}@{host}/{db}')\n",
    "df_pandas = pd.read_sql(sql=query, con=engine\n",
    "                        # , chunksize=10000\n",
    "                        )\n",
    "\n",
    "# print(f'chunks size: {sys.getsizeof(chunks)}')\n",
    "# logging.info(f\"chunks loaded {sys.getsizeof(chunks)}\")\n",
    "\n",
    "# df_pandas = pd.DataFrame()\n",
    "# for i in tqdm(range(sys.getsizeof(chunks)), desc='Reading from View'):\n",
    "#     for chunk in chunks:\n",
    "#         view_df = pd.concat([df_pandas, chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: for comparisons  planetscale limitations of 20sec and 100000 rows\n",
    "\n",
    "- MySql Workbench fetch `query` in less than **15sec** for 648648 rows\n",
    "- connectorx fetch `query` in less than \n",
    "  - **18sec** for 648648 rows, with no partitioning\n",
    "  - **40sec** for 648648 rows, using partitions=10\n",
    "  - **1min 50sec** for 648648 rows, using partitions=100\n",
    "- sqlalchemy+pandas fetch `query` in less than **20sec** for 648648 rows, without using chunks\n",
    "\n",
    "Need to verify IDs with original train.csv. Did our database schema auto-increment everytime we run a fetch with \"Select\" or upon \"create view\"? See `id` columns in `df` versus `df_pandas` is different.\n",
    "\n",
    "df 2nd run: id range from 233706 - 1323002 (3rd run with 0 partitions has same IDs)\n",
    "df_pandas 1st run: id range from 0 - 1945943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF loaded confirm: 1972674 rows × 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt='select * from VwDump1'\n",
    "\n",
    "chunks = db.load(query=stmt)\n",
    "\n",
    "print(f'VwDump1 chunks size: {sys.getsizeof(chunks)}')\n",
    "logging.info(f\"VwDump1 chunks loaded {sys.getsizeof(chunks)}\")\n",
    "\n",
    "view_df = pd.DataFrame()\n",
    "for i in tqdm(range(sys.getsizeof(chunks)), desc='Reading from View'):\n",
    "    for chunk in chunks:\n",
    "        view_df = pd.concat([view_df, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF loaded confirm: 3000888 rows × 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "view_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out data ingestion with connectorx library\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create engine to talk to database\n",
    "engine = create_engine(\n",
    "    f'mysql+pymysql://'             # dialect + driver\n",
    "    f'{config.get(\"USERNAME\")}'     # username\n",
    "    f':{config.get(\"PASSWORD\")}'    # password\n",
    "    f'@{config.get(\"ENDPOINT\")}'    # host\n",
    "    f':{config.get(\"PORT\")}'        # port\n",
    "    f'/{config.get(\"DBNAME\")}'      # database\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish connection and make the query\n",
    "with engine.connect() as cnxn:\n",
    "    with open('../src/scripts/query_data.sql') as f:\n",
    "        query = text(f.read())\n",
    "        results = pd.read_sql(query, cnxn)\n",
    "\n",
    "# runtime 1 min 1.4 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = os.getenv(\"PS_HOST\")\n",
    "DB_USERNAME = os.getenv(\"PS_USERNAME\")\n",
    "DB_PASSWORD = os.getenv(\"PS_PASSWORD\")\n",
    "DB_DATABASE = os.getenv(\"PS_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    host=DB_HOST,\n",
    "    user=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    database=DB_DATABASE,\n",
    "    # ssl_verify_identity=True,\n",
    "    # ssl_ca=\"/etc/ssl/certs/ca-certificates.crt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "\n",
    "batch_size = 100000\n",
    "start_id = 0\n",
    "rows = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    cursor.execute(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM train\n",
    "        WHERE id >= {start_id}\n",
    "        ORDER BY id\n",
    "        LIMIT {batch_size}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    batch = cursor.fetchall()\n",
    "    if not batch:\n",
    "        break\n",
    "    \n",
    "    rows.extend(batch)\n",
    "    start_id = batch[-1][0] + 1\n",
    "    \n",
    "    if len(rows) >= 3000000:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    host=DB_HOST,\n",
    "    user=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    database=DB_DATABASE,\n",
    "    # ssl_verify_identity=True,\n",
    "    # ssl_ca=\"/etc/ssl/certs/ca-certificates.crt\"\n",
    "    connect_timeout=1000\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SET GLOBAL connect_timeout=60;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SET WORKLOAD = 'olap';\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SELECT * FROM full_df\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(len(rows))\n",
    "\n",
    "df = pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df.index.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_store = df.groupby(by=['store_nbr', 'family'], group_keys=True).agg('sum', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_store.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, tsmode=True, title=\"Time-Series EDA Quito City\")\n",
    "# profile.to_notebook_iframe()\n",
    "# profile.to_file(\"../artifacts/reports/quito_ProfileReport.html\") # AttributeError: 'float' object has no attribute 'shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(11, 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sales_sum'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_plot = ['onpromotion_sum', 'transactions_sum', 'sales_sum']\n",
    "axes = df[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11, 9), subplots=True)\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Daily Totals (GWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013 = df.loc['2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_2013.loc['2013', 'sales_sum'].plot()\n",
    "ax.set_ylabel('Daily Sales for 2013');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='weekday', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='month', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='store_nbr', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.store_nbr.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- have outliers needing treatment; need to treat outliers first before we can analyse seasonality for `weekday` and `month`\n",
    "- selected Store 44 as it has most spread for `transaction_sum` meaning more activity; note assumption that 0 transaction means store is closed as there's no sale on that day\n",
    "- which tracks with 0 transactions occuring on days with holidays (not included to keep dataset small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str44 = df[(df.store_nbr == 44)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44 = df_str44.groupby(by=['date'], group_keys=True).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44.drop(columns=['id','store_nbr','year','month','day_of_month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44 = grp_df_str44.asfreq('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoML with pycaret\n",
    "\n",
    "EDA and ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check installed version\n",
    "import pycaret\n",
    "pycaret.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pycaret time series and init setup\n",
    "from pycaret.time_series import *\n",
    "s = setup(grp_df_str44,  \n",
    "            target='sales_sum', \n",
    "            fh = 28, \n",
    "            session_id = 123, \n",
    "            profile=True,\n",
    "            numeric_imputation_exogenous='mean',\n",
    "            numeric_imputation_target=\"median\",\n",
    "            # ignore_features = ['id', 'family', 'store_nbr']\n",
    "          )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistical tests on original data\n",
    "check_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources for add_metric()\n",
    "\n",
    "- https://towardsdatascience.com/predict-customer-churn-the-right-way-using-pycaret-8ba6541608ac\n",
    "- https://github.com/pycaret/pycaret/issues/3491\n",
    "- https://github.com/pycaret/pycaret/issues/1063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# create a custom function\n",
    "def rmsle(y_true, y_pred):\n",
    "    return mean_squared_log_error(y_true\n",
    "                        , y_pred\n",
    "                        , squared=False)\n",
    "\n",
    "add_metric('msle', 'MSLE', mean_squared_log_error, greater_is_better=False) # default squared=True\n",
    "add_metric('rmsle', 'RMSLE', rmsle, greater_is_better=False) # for problem statement squared=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare baseline models\n",
    "best = compare_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
