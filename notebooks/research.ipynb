{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales - Time Series Forecasting\n",
    "\n",
    "Use machine learning to predict grocery sales. [source](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview/description)\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this Kaggle competition, the goal is to \n",
    "\n",
    "> build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.\n",
    "\n",
    "The evaluation metric for this competition is ***Root Mean Squared Logarithmic Error***.\n",
    "\n",
    "The `RMSLE` is calculated as:\n",
    "\n",
    "$$\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ n $ is the total number of instances,\n",
    "     \n",
    "- $\\hat{y}$ is the predicted value of the target for instance (i),\n",
    "   \n",
    "- $y_i$ is the actual value of the target for instance (i), and,\n",
    " \n",
    "- $log$ is the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each id in the test set, you must predict a value for the sales variable. The file should contain a header and have the following format:\n",
    "\n",
    "    ```\n",
    "    id,sales\n",
    "    3000888,0.0\n",
    "    3000889,0.0\n",
    "    3000890,0.0\n",
    "    3000891,0.0\n",
    "    3000892,0.0\n",
    "    etc.\n",
    "    ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries for this research notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra libraries for this version of notebook:\n",
    "\n",
    "via below conda or pip install (or copy+paste into Terminal):\n",
    "- ydata-profiling\n",
    "- tqdm\n",
    "- connectorx\n",
    "- pycaret\n",
    "\n",
    "needs to be installed separately:\n",
    "- [pscale cli](https://github.com/planetscale/cli#installation) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install ydata-profiling tqdm connectorx pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ydata-profiling tqdm connectorx pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_file_path is /Users/galvangoh/Desktop/Tech/ai_practitioners/time-series-forecasting/time-series-forecasting/notebooks\n",
      "src_folder_path is /Users/galvangoh/Desktop/Tech/ai_practitioners/time-series-forecasting/time-series-forecasting/src\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# to overcome path issue for src\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# set the path to the current file\n",
    "current_file_path = Path().resolve()\n",
    "print(f\"current_file_path is {current_file_path}\")\n",
    "\n",
    "# set the path to the src folder\n",
    "src_folder_path = current_file_path.parent / 'src'\n",
    "print(f\"src_folder_path is {src_folder_path}\")\n",
    "\n",
    "# add the src folder to the system path\n",
    "sys.path.append(str(src_folder_path))\n",
    "\n",
    "from data_loader import DBDataLoader\n",
    "from logger import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlalchemy.__version__  = '1.4.40'\n",
      "pandas.__version__  = '1.5.3'\n",
      "polars.__version__  = '0.18.9'\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import pandas\n",
    "import polars\n",
    "print(f'{sqlalchemy.__version__  = }')\n",
    "print(f'{pandas.__version__  = }')\n",
    "print(f'{polars.__version__  = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Analysis\n",
    "\n",
    "Making sure that the csv files imported to SQL server is identical to localhost and planetscale `time_series` database.\n",
    "\n",
    "### Dates in train.csv\n",
    "\n",
    "```sql \n",
    "select count(*) from train;\n",
    "``` \n",
    "\n",
    "|count(*)|\n",
    "|--------|\n",
    "| 3000888|\n",
    "\n",
    "```sql\n",
    "select\n",
    "    min(tr.`date`) as oldest_date_train, \n",
    "    max(tr.`date`) as newest_date_train\n",
    "from\n",
    "    train as tr;\n",
    "```\n",
    "\n",
    "|oldest_date_train|newest_date_train|\n",
    "|-----------------|-----------------|\n",
    "|       2013-01-01|       2017-08-15|\n",
    "\n",
    "\n",
    "### Dates in test.csv\n",
    "\n",
    "```sql \n",
    "select count(*) from test;\n",
    "``` \n",
    "\n",
    "|count(*)|\n",
    "|--------|\n",
    "|   28512|\n",
    "\n",
    "\n",
    "```sql\n",
    "select\n",
    "    min(ts.`date`) as oldest_date_test, \n",
    "    max(ts.`date`) as newest_date_test\n",
    "from\n",
    "    test as ts;\n",
    "```\n",
    "\n",
    "|oldest_date_test|newest_date_test|\n",
    "|----------------|----------------|\n",
    "|      2017-08-16|      2017-08-31|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempt at overcoming pscale limitationns\n",
    "\n",
    "Due to the *free tier* on [remote database on planetscale](https://app.planetscale.com/ai-practitioners) imposing limitations of query returns of 100,000 records and execution time of 20s, we have further reduced the returned results of the query and then saved as SQL Views for easier and faster retrieval.\n",
    "\n",
    "row counts of each table or view:\n",
    "\n",
    "|table / view | records count| description\n",
    "|-----------------|-----------------|-----------------|\n",
    "| train.csv | 3,000,888 | full count of raw train.csv |\n",
    "| full_df | 3,054,348 | full df after join of 5 tables | \n",
    "| quito | 1,000,296 | join of train, store & transaction and pull data for `quito` city for all years and stores |\n",
    "| sales_year_state | 818,021 | sales statistics from `full_df` groupedby year, state, family, sales | \n",
    "| year_2013 | 648,648 | join of train, store & transaction and pull data for `2013` year for all cities and stores | \n",
    "| quito_2013 | 216,216 | join of train, store & transaction and pull data for `2013` year and `quito`  city for all stores | \n",
    "| quito_44 | 36,036 | join of train, store & transaction and pull data for `quito` city and store_nbr 44 for all the years | \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Query data from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''SELECT * FROM train'''\n",
    "CHUNKSIZE=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) on localhost using connextorx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] localhost Connection Successful using connectorx\n"
     ]
    }
   ],
   "source": [
    "conn=DBDataLoader().get_connection_string(\"local\", \"connectorx\")  # no need to swap to \"remote\" as ConnectorX does not support remote connections\n",
    "print(\"[+] localhost Connection Successful using connectorx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "df=cx.read_sql(conn, query) #, partition_on=\"id\", partition_num=10)\n",
    "print(f'{df.shape = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) on localhost using sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(DBDataLoader().get_connection_string(\"local\", \"sqlalchemy\")) # swap to \"remote\" for accessing planetscale\n",
    "print(\"[+] localhost Connection Successful using sqlalchemy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "df = pd.read_sql(sql=query, con=engine)\n",
    "print(f'{df.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "chunks = pd.read_sql(sql=query, con=engine, chunksize=CHUNKSIZE)\n",
    "# print(f'{df.shape = }')\n",
    "# print(f'chunks size: {sys.getsizeof(chunks)}')\n",
    "# logging.info(f\"chunks loaded {sys.getsizeof(chunks)}\")\n",
    "\n",
    "df_pandas = pd.DataFrame()\n",
    "# for i in tqdm(range(sys.getsizeof(chunks)), desc='Reading from View'):\n",
    "for chunk in chunks:\n",
    "    df_pandas = pd.concat([df_pandas, chunk])\n",
    "\n",
    "print(f'{df_pandas.shape = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) on localhost using sqlalchemy + polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "import polars as pl\n",
    "query = \"SELECT * FROM train\"\n",
    "df = pl.read_database(query=query, connection=conn, engine=\"connectorx\")\n",
    "print(f'{df.shape = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on chunking / partitioning / yield_per\n",
    "\n",
    "https://planetscale.com/blog/using-mysql-with-sql-alchemy-hands-on-examples\n",
    "\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "connection_string = \"mysql+mysqlconnector://user1:pscale_pw_abc123@us-east.connect.psdb.cloud:3306/sqlalchemy\"\n",
    "engine = create_engine(connection_string, echo=True)\n",
    "```\n",
    "\n",
    "> By default, SSL/TLS usage in mysql-connector-python is enabled, which is required to connect to PlanetScale. This means you do not need to pass it into create_engine() as a connection arguement.\n",
    "\n",
    "https://docs.sqlalchemy.org/en/20/orm/queryguide/api.html#orm-queryguide-yield-per\n",
    "https://docs.sqlalchemy.org/en/20/core/connections.html#engine-stream-results\n",
    "\n",
    "```python\n",
    "with engine.connect() as conn:\n",
    "    with conn.execution_options(yield_per=100).execute(\n",
    "        text(\"select * from table\")\n",
    "    ) as result:\n",
    "        for partition in result.partitions():\n",
    "            # partition is an iterable that will be at most 100 items\n",
    "            for row in partition:\n",
    "                print(f\"{row}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) on remote using sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote access to planetscale\n",
    "engine = create_engine(DBDataLoader().get_connection_string(\"remote\", \"sqlalchemy\")) # swap to \"remote\" for accessing planetscale\n",
    "print(\"[+] planetscale Connection Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"SET WORKLOAD = 'olap'\")\n",
    "    result = conn.execute(query)\n",
    "    rows = result.fetchall()\n",
    "    df = pd.DataFrame(rows, columns=result.keys())\n",
    "\n",
    "print(f'{df.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# initialize empty list to store dataframes\n",
    "dfs = []\n",
    "# loop through chunks of data and append to list\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(\"SET WORKLOAD = 'olap'\")\n",
    "    result = conn.execute(query).yield_per(CHUNKSIZE)\n",
    "    for chunk in iter(lambda: result.fetchmany(CHUNKSIZE), []):\n",
    "        dfs.append(pd.DataFrame(chunk))\n",
    "        # print(f'chunk#{len(dfs)}')\n",
    "\n",
    "# concatenate dataframes into one dataframe\n",
    "big_df = pd.concat(dfs)\n",
    "print(f'{big_df.shape = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) on remote using sqlalchemy + polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# df_polars = pl.read_database(query=query, connection_uri=get_connection_string(\"remote\", \"connectorx\"), engine=\"connectorx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2.2) on remote using sqlalchemy + polars is a NO GO\n",
    "\n",
    "MySqlError { ERROR 1105 (HY000): unknown error: Code: UNAVAILABLE\n",
    "    server does not allow insecure connections, client must use SSL/TLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Notes: for comparisons  planetscale limitations of 20sec and 100000 rows. Times are not exact, of course, each run would have +-. \n",
    "\n",
    "### `time_series` on localhost\n",
    "- MySql Workbench fetch `query` in less than **5.969 s / 0.203 s (duration/fetch)** for 216,216 rows × 10 columns\n",
    "- connectorx+pandas fetch `query` in **13.1 s ± 769 ms per loop** for 216,216 rows, with no partitioning\n",
    "- sqlalchemy+pandas fetch `query` in **7.18 s ± 79.7 ms per loop** for 216,216 rows, without using chunks\n",
    "- sqlalchemy+pandas fetch `query` in **7.32 s ± 43.5 ms per loop** for 216,216 rows, with CHUNKSIZE=100000\n",
    "- connectorx+polars fetch `query` in **6.44 s ± 82.1 ms per loop** for 216,216 rows, without using chunks\n",
    "\n",
    "### `time_series` on planetscale\n",
    "- connectorx does not work on remote\n",
    "- MySql Workbench fetch `query` : hit the 100,000 rows limit just before 20sec mark\n",
    "- sqlalchemy(mysqldb)+pandas : got the \"OperationalError: (MySQLdb.OperationalError) (1105, 'charset/name utf8mb3 is not supported')\"\n",
    "- sqlalchemy(mysqlconnector)+pandas fetch `query` in **11.1 s ± 529 ms per loop** for 216,216 rows, without using chunks\n",
    "- sqlalchemy(mysqlconnector)+pandas fetch `query` in **11.5 s ± 624 ms per loop** for 216,216 rows, with CHUNKSIZE=100000\n",
    "- sqlalchemy(connectorx)+polars fetch `query` ERROR *client must use SSL/TLS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"split df by city code\" to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "select \n",
    "    state, city, \n",
    "    `year`, month, day_of_month,\n",
    "    count(id),\n",
    "    coalesce(sum(sales), 0) as sales_total\n",
    "from full_df\n",
    "group by state, city, `year`, month, day_of_month\n",
    "order by `year` asc, `month` asc, day_of_month asc\n",
    "'''\n",
    "\n",
    "# set the path to the data folder\n",
    "data_folder_path = current_file_path.parent / 'data'\n",
    "print(f\"data_folder_path is {data_folder_path}\")\n",
    "\n",
    "df = pd.read_sql(sql=query, con=conn)\n",
    "\n",
    "for city, group in df.groupby('city'):\n",
    "    filename = data_folder_path / f'{city}.csv'\n",
    "    group.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still to review below onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF loaded confirm: 1972674 rows × 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt='select * from VwDump1'\n",
    "\n",
    "chunks = db.load(query=stmt)\n",
    "\n",
    "print(f'VwDump1 chunks size: {sys.getsizeof(chunks)}')\n",
    "logging.info(f\"VwDump1 chunks loaded {sys.getsizeof(chunks)}\")\n",
    "\n",
    "view_df = pd.DataFrame()\n",
    "for i in tqdm(range(sys.getsizeof(chunks)), desc='Reading from View'):\n",
    "    for chunk in chunks:\n",
    "        view_df = pd.concat([view_df, chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF loaded confirm: 3000888 rows × 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "view_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out data ingestion with connectorx library\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create engine to talk to database\n",
    "engine = create_engine(\n",
    "    f'mysql+pymysql://'             # dialect + driver\n",
    "    f'{config.get(\"USERNAME\")}'     # username\n",
    "    f':{config.get(\"PASSWORD\")}'    # password\n",
    "    f'@{config.get(\"ENDPOINT\")}'    # host\n",
    "    f':{config.get(\"PORT\")}'        # port\n",
    "    f'/{config.get(\"DBNAME\")}'      # database\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish connection and make the query\n",
    "with engine.connect() as cnxn:\n",
    "    with open('../src/scripts/query_data.sql') as f:\n",
    "        query = text(f.read())\n",
    "        results = pd.read_sql(query, cnxn)\n",
    "\n",
    "# runtime 1 min 1.4 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = os.getenv(\"PS_HOST\")\n",
    "DB_USERNAME = os.getenv(\"PS_USERNAME\")\n",
    "DB_PASSWORD = os.getenv(\"PS_PASSWORD\")\n",
    "DB_DATABASE = os.getenv(\"PS_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    host=DB_HOST,\n",
    "    user=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    database=DB_DATABASE,\n",
    "    # ssl_verify_identity=True,\n",
    "    # ssl_ca=\"/etc/ssl/certs/ca-certificates.crt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "\n",
    "batch_size = 100000\n",
    "start_id = 0\n",
    "rows = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "    cursor.execute(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM train\n",
    "        WHERE id >= {start_id}\n",
    "        ORDER BY id\n",
    "        LIMIT {batch_size}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    batch = cursor.fetchall()\n",
    "    if not batch:\n",
    "        break\n",
    "    \n",
    "    rows.extend(batch)\n",
    "    start_id = batch[-1][0] + 1\n",
    "    \n",
    "    if len(rows) >= 3000000:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    host=DB_HOST,\n",
    "    user=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    database=DB_DATABASE,\n",
    "    # ssl_verify_identity=True,\n",
    "    # ssl_ca=\"/etc/ssl/certs/ca-certificates.crt\"\n",
    "    connect_timeout=1000\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SET GLOBAL connect_timeout=60;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SET WORKLOAD = 'olap';\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cursor.execute(\n",
    "    f\"\"\"\n",
    "    SELECT * FROM full_df\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "print(len(rows))\n",
    "\n",
    "df = pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df.index.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_store = df.groupby(by=['store_nbr', 'family'], group_keys=True).agg('sum', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_store.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, tsmode=True, title=\"Time-Series EDA Quito City\")\n",
    "# profile.to_notebook_iframe()\n",
    "# profile.to_file(\"../artifacts/reports/quito_ProfileReport.html\") # AttributeError: 'float' object has no attribute 'shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(11, 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sales_sum'].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_plot = ['onpromotion_sum', 'transactions_sum', 'sales_sum']\n",
    "axes = df[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11, 9), subplots=True)\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Daily Totals (GWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013 = df.loc['2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_2013.loc['2013', 'sales_sum'].plot()\n",
    "ax.set_ylabel('Daily Sales for 2013');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='weekday', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='month', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(11, 10), sharex=True)\n",
    "for name, ax in zip(cols_plot, axes):\n",
    "    sns.boxplot(data=df, x='store_nbr', y=name, ax=ax)\n",
    "    ax.set_ylabel('Sum')\n",
    "    ax.set_title(name)\n",
    "    # Remove the automatic x-axis label from all but the bottom subplot\n",
    "    if ax != axes[-1]:\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.store_nbr.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- have outliers needing treatment; need to treat outliers first before we can analyse seasonality for `weekday` and `month`\n",
    "- selected Store 44 as it has most spread for `transaction_sum` meaning more activity; note assumption that 0 transaction means store is closed as there's no sale on that day\n",
    "- which tracks with 0 transactions occuring on days with holidays (not included to keep dataset small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str44 = df[(df.store_nbr == 44)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44 = df_str44.groupby(by=['date'], group_keys=True).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44.drop(columns=['id','store_nbr','year','month','day_of_month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44 = grp_df_str44.asfreq('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoML with pycaret\n",
    "\n",
    "EDA and ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df_str44.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check installed version\n",
    "import pycaret\n",
    "pycaret.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pycaret time series and init setup\n",
    "from pycaret.time_series import *\n",
    "s = setup(grp_df_str44,  \n",
    "            target='sales_sum', \n",
    "            fh = 28, \n",
    "            session_id = 123, \n",
    "            profile=True,\n",
    "            numeric_imputation_exogenous='mean',\n",
    "            numeric_imputation_target=\"median\",\n",
    "            # ignore_features = ['id', 'family', 'store_nbr']\n",
    "          )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistical tests on original data\n",
    "check_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources for add_metric()\n",
    "\n",
    "- https://towardsdatascience.com/predict-customer-churn-the-right-way-using-pycaret-8ba6541608ac\n",
    "- https://github.com/pycaret/pycaret/issues/3491\n",
    "- https://github.com/pycaret/pycaret/issues/1063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# create a custom function\n",
    "def rmsle(y_true, y_pred):\n",
    "    return mean_squared_log_error(y_true\n",
    "                        , y_pred\n",
    "                        , squared=False)\n",
    "\n",
    "add_metric('msle', 'MSLE', mean_squared_log_error, greater_is_better=False) # default squared=True\n",
    "add_metric('rmsle', 'RMSLE', rmsle, greater_is_better=False) # for problem statement squared=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare baseline models\n",
    "best = compare_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
