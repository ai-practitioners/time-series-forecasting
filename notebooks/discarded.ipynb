{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = dotenv_values()\n",
    "\n",
    "# host = config.get('ps_host')\n",
    "# port = int(config.get('ps_port'))\n",
    "# user = config.get('ps_username')\n",
    "# password = config.get('ps_password')\n",
    "# db = config.get('ps_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# DB_HOST = os.getenv(\"ps_host\")\n",
    "# DB_USERNAME = os.getenv(\"ps_username\")\n",
    "# DB_PASSWORD = os.getenv(\"ps_password\")\n",
    "# DB_DATABASE = os.getenv(\"ps_database\")\n",
    "\n",
    "DB_HOST = os.getenv(\"ENDPOINT\")\n",
    "DB_USERNAME = os.getenv(\"USERNAME\")\n",
    "DB_PASSWORD = os.getenv(\"PASSWORD\")\n",
    "DB_DATABASE = os.getenv(\"DBNAME\")\n",
    "\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=DB_HOST,\n",
    "    user=DB_USERNAME,\n",
    "    password=DB_PASSWORD,\n",
    "    database=DB_DATABASE,\n",
    "    port=3306\n",
    "    # ssl_verify_identity=True,\n",
    "    # ssl_ca=\"path/to/ssl_cert\"\n",
    ")\n",
    "\n",
    "print(\"Connected to the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "# Define the query\n",
    "# query = \"SELECT * FROM train\"\n",
    "\n",
    "# Execute the query in chunks of 100000 records\n",
    "chunk_size = 10000\n",
    "offset = 0\n",
    "dfs = []\n",
    "while True:\n",
    "    # Execute the query\n",
    "    cursor.execute(query + f\" LIMIT {chunk_size} OFFSET {offset}\")\n",
    "    # Fetch the results\n",
    "    results = cursor.fetchall()\n",
    "    # If there are no more results, break out of the loop\n",
    "    if not results:\n",
    "        break\n",
    "    # Convert the results to a DataFrame and append it to the list of DataFrames\n",
    "    dfs.append(pd.DataFrame(results))\n",
    "    # Increment the offset\n",
    "    offset += chunk_size\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "DB_HOST = os.getenv(\"ENDPOINT\")\n",
    "DB_USERNAME = os.getenv(\"USERNAME\")\n",
    "DB_PASSWORD = os.getenv(\"PASSWORD\")\n",
    "DB_DATABASE = os.getenv(\"DBNAME\")\n",
    "logging.info(\"dotenv loaded into config successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sqlalchemy` with\n",
    "`create_engine(f'mysql+mysqldb://{user}:{password}@{host}/{db}')`\n",
    "\n",
    "OperationalError: (1105, 'charset/name utf8mb3 is not supported')\n",
    "\n",
    "The above exception was the direct cause of the following exception:\n",
    "\n",
    "OperationalError                          Traceback (most recent call last)\n",
    "/tmp/ipykernel_161486/2004886828.py in ()\n",
    "      3 \n",
    "      4 db = DBDataLoader()\n",
    "----> 5 stores = db.load() # default stores table\n",
    "\n",
    "OperationalError: (MySQLdb.OperationalError) (1105, 'charset/name utf8mb3 is not supported')\n",
    "(Background on this error at: https://sqlalche.me/e/14/e3q8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import connectorx as cx\n",
    "conn=\"mysql://{user}:{pw}@{host}:{port}/{db}\".format(user=DB_USERNAME,\n",
    "                                                       pw=DB_PASSWORD,\n",
    "                                                       host=DB_HOST,\n",
    "                                                       db=DB_DATABASE,\n",
    "                                                       port=3306,\n",
    "                                                    #    ssl_verify_identity=True,\n",
    "                                                    #    ssl_ca=\"path/to/ssl_cert\"\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data set using .sql file\n",
    "query_file_path = '../src/scripts/train_store_hols.sql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "select\n",
    "\ttr.`date`, tr.family, sum(tr.sales), sum(tr.onpromotion), \n",
    "\ttr.store_nbr, st.city,\n",
    "\tYEAR(tr.`date`) `year`,\n",
    "    MONTH(tr.`date`) `month`,\n",
    "    DAY(tr.`date`) `day_of_month`,\n",
    "\tsum(transactions)\n",
    "from\n",
    "\ttrain as tr\n",
    "left join stores as st\n",
    "    on tr.store_nbr = st.store_nbr\n",
    "left join transactions txn \n",
    "    on tr.`date` = txn.`date` AND tr.store_nbr = txn.store_nbr\n",
    "where\n",
    " \tyear(tr.`date`) between 2013 and 2015\n",
    "group by tr.`date`, tr.family, st.city, tr.store_nbr\n",
    "having st.city = 'Quito'\n",
    "order by tr.`date`\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(query_file_path, 'r') as query:\n",
    "    chunks = db.load(query=query.read())\n",
    "    # count += 1\n",
    "    print(f'chunks size: {sys.getsizeof(chunks)}')\n",
    "    logging.info(f\"chunks loaded {sys.getsizeof(chunks)}\")\n",
    "    df = pd.DataFrame()\n",
    "    for i in tqdm(range(sys.getsizeof(chunks)), desc='Reading from DB'):\n",
    "        for chunk in chunks:\n",
    "            df = pd.concat([df, chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stmt='select * from full_df'\n",
    "DF loaded confirm: 3000888 rows × 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stmt='select * from quito_frm_view'\n",
    "\n",
    "chunks = db.load() # query=stmt\n",
    "\n",
    "print(f'VwDump1 chunks size: {sys.getsizeof(chunks)}')\n",
    "logging.info(f\"VwDump1 chunks loaded {sys.getsizeof(chunks)}\")\n",
    "\n",
    "view_df = pd.DataFrame()\n",
    "for i in tqdm(range(sys.getsizeof(chunks)), desc='Reading from View'):\n",
    "    for chunk in chunks:\n",
    "        view_df = pd.concat([view_df, chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'SELECT * FROM VwDump1'\n",
    "DF loaded confirm: 1972674 rows × 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# from ydata_profiling.utils.cache import cache_file\n",
    "# from ydata_profiling import ProfileReport\n",
    "\n",
    "# file_name = cache_file(\n",
    "#     \"pollution_us_2000_2016.csv\",\n",
    "#     \"https://query.data.world/s/mz5ot3l4zrgvldncfgxu34nda45kvb\",\n",
    "# )\n",
    "\n",
    "# df = pd.read_csv(file_name, index_col=[0])\n",
    "\n",
    "# # Filtering time-series to profile a single site\n",
    "# site = df[df[\"Site Num\"] == 3003]\n",
    "\n",
    "# # Setting what variables are time series\n",
    "# type_schema = {\n",
    "#     \"NO2 Mean\": \"timeseries\",\n",
    "#     \"NO2 1st Max Value\": \"timeseries\",\n",
    "#     \"NO2 1st Max Hour\": \"timeseries\",\n",
    "#     \"NO2 AQI\": \"timeseries\",\n",
    "#     \"cos\": \"numeric\",\n",
    "#     \"cat\": \"numeric\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice Job Thomas and all this on a Thursday!\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/73319779\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "days_data = pl.DataFrame({\n",
    "    \"weekday\": [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"],\n",
    "    \"date\": [\"1900-01-01\", \"1900-01-02\", \"1900-01-03\", \"1900-01-04\", \"1900-01-05\"]\n",
    "})\n",
    "\n",
    "sales_data = pl.DataFrame({\n",
    "    \"sales\": [1000, 20, 300, 40000, 555],\n",
    "    \"rep\": [\"Joe\", \"Mary\", \"Robert\", \"Thomas\", \"Susanna\"],\n",
    "    \"date\": [\"1900-01-01\", \"1900-01-02\", \"1900-01-03\", \"1900-01-04\", \"1900-01-05\"]\n",
    "})\n",
    "\n",
    "joined = sales_data.join(days_data, on=\"date\")\n",
    "\n",
    "# filtered = joined[lambda df: df[\"sales\"] > 10000]\n",
    "\n",
    "filtered = joined.filter(\n",
    "                (pl.col(\"sales\") > 10000)\n",
    "            )\n",
    "\n",
    "for row in filtered.iter_rows(named=True):\n",
    "    print(f\"Nice Job {row['rep']} and all this on a {row['weekday']}!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
